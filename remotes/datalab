#!/usr/bin/env Rscript
suppressPackageStartupMessages(library("rjson"))
suppressPackageStartupMessages(library("multicore"))
# Execute commands in config/queue in parallel across datalab machines.  
# Finds number of open cores for each machine and schedules accordingly.

config <- fromJSON(,"config.json")

# This looks potentially relevant to what we're trying to do here.
# http://www.gnu.org/software/parallel/man.html#example__gnu_parallel_as_queue_system_batch_manager

# For each datalab machine
hosts <- paste("d",c(12:10,8:5),sep="")  # prefer newer datalabs, skip wackjob number 9
number.used <- unlist(mclapply(hosts, function(h) {
  command <- paste('ssh ',h,' \"ps -eo pcpu,pid,user | sort -r | head -9\" ',sep='')
  x <- read.table(pipe(command),header=TRUE)
  num <- sum(x$X.CPU > 50)
#  cat(h,":",num,"\n")
  return(num)
}))

#NOTE:               cpu utilization of the process in "##.#" format.
#                    Currently, it is the CPU time used divided by the time the
#                    process has been running (cputime/realtime ratio),
#                    expressed as a percentage. It will not add up to 100%
#                    unless you are lucky. (alias pcpu).
# I've seen it vary from what top reports for long running processes.

ix <- which(number.used < 8)
remotes <- paste(8-number.used[ix],hosts[ix],sep="/",collapse=",")
cat("Spinning:",remotes,"\n")
command <- paste("cd ",config$path,"; cat queue | ./pipeline/parallel --sshlogin",remotes)
cat("Executing:",command,"\n")
system(command)
