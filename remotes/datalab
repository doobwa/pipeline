#!/usr/bin/env Rscript
suppressPackageStartupMessages(library("rjson"))
# Execute commands in config/queue in parallel across datalab machines.  
# Finds number of open cores for each machine and schedules accordingly.

config <- fromJSON(,"config.json")

# This looks potentially relevant to what we're trying to do here.
# http://www.gnu.org/software/parallel/man.html#example__gnu_parallel_as_queue_system_batch_manager

# For each datalab machine
hosts <- paste("d",c(12:10,8:5),sep="")  # prefer newer datalabs
require(multicore)
number.used <- unlist(mclapply(hosts, function(h) {
  command <- paste('ssh ',h,' \"ps -eo pcpu,pid,user | sort -r | head -9\" ',sep='')
  x <- read.table(pipe(command),header=TRUE)
  num <- sum(x$X.CPU > 50)
  cat(h,":",num,"\n")
  return(num)
}))

#NOTE:               cpu utilization of the process in "##.#" format.
#                    Currently, it is the CPU time used divided by the time the
#                    process has been running (cputime/realtime ratio),
#                    expressed as a percentage. It will not add up to 100%
#                    unless you are lucky. (alias pcpu).
# I've seen it vary from what top reports for long running processes.

remotes <- paste(8-number.used,hosts,sep="/",collapse=",")
command <- paste("cd ",config$path,"; cat queue | ./pipeline/parallel --sshlogin",remotes)
print(command)
system(command)
